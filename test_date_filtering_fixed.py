#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æ—¶é—´è¿‡æ»¤åŠŸèƒ½
éªŒè¯å„ä¸ªæœç´¢å¼•æ“çš„æ—¶é—´è¿‡æ»¤æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ai_news_collector_lib'))

from ai_news_collector_lib.config.settings import SearchConfig
from ai_news_collector_lib.core.collector import AINewsCollector
from ai_news_collector_lib.models.article import Article

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DateFilteringTester:
    """æ—¶é—´è¿‡æ»¤åŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config = SearchConfig()
        self.collector = AINewsCollector(self.config)
        
    def analyze_article_dates(self, articles: List[Article], days_back: int) -> Dict[str, Any]:
        """åˆ†ææ–‡ç« æ—¥æœŸåˆ†å¸ƒ"""
        now = datetime.now(timezone.utc)
        cutoff_date = now - timedelta(days=days_back)
        
        analysis = {
            "total_articles": len(articles),
            "within_range": 0,
            "outside_range": 0,
            "invalid_dates": 0,
            "date_distribution": {},
            "oldest_article": None,
            "newest_article": None,
            "date_issues": []
        }
        
        valid_articles = []
        
        for article in articles:
            try:
                if not article.published:
                    analysis["invalid_dates"] += 1
                    continue
                
                # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
                published_str = article.published
                if published_str.endswith("Z"):
                    published_str = published_str[:-1] + "+00:00"
                
                published_time = datetime.fromisoformat(published_str)
                valid_articles.append((article, published_time))
                
                # ç»Ÿè®¡æ—¥æœŸåˆ†å¸ƒ
                days_old = (now - published_time).days
                if days_old <= 1:
                    analysis["date_distribution"]["0-1å¤©"] = analysis["date_distribution"].get("0-1å¤©", 0) + 1
                elif days_old <= 7:
                    analysis["date_distribution"]["1-7å¤©"] = analysis["date_distribution"].get("1-7å¤©", 0) + 1
                elif days_old <= 30:
                    analysis["date_distribution"]["7-30å¤©"] = analysis["date_distribution"].get("7-30å¤©", 0) + 1
                elif days_old <= 365:
                    analysis["date_distribution"]["30-365å¤©"] = analysis["date_distribution"].get("30-365å¤©", 0) + 1
                else:
                    analysis["date_distribution"]["1å¹´ä»¥ä¸Š"] = analysis["date_distribution"].get("1å¹´ä»¥ä¸Š", 0) + 1
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                if published_time >= cutoff_date:
                    analysis["within_range"] += 1
                else:
                    analysis["outside_range"] += 1
                    analysis["date_issues"].append({
                        "title": article.title[:50] + "..." if len(article.title) > 50 else article.title,
                        "source": article.source,
                        "published": article.published,
                        "days_old": days_old
                    })
                
            except (ValueError, TypeError) as e:
                analysis["invalid_dates"] += 1
                analysis["date_issues"].append({
                    "title": article.title[:50] + "..." if len(article.title) > 50 else article.title,
                    "source": article.source,
                    "error": f"æ—¥æœŸè§£æå¤±è´¥: {e}",
                    "published": article.published
                })
        
        # æ‰¾åˆ°æœ€è€å’Œæœ€æ–°çš„æ–‡ç« 
        if valid_articles:
            valid_articles.sort(key=lambda x: x[1])
            analysis["oldest_article"] = {
                "title": valid_articles[-1][0].title[:50] + "..." if len(valid_articles[-1][0].title) > 50 else valid_articles[-1][0].title,
                "source": valid_articles[-1][0].source,
                "published": valid_articles[-1][0].published,
                "days_old": (now - valid_articles[-1][1]).days
            }
            analysis["newest_article"] = {
                "title": valid_articles[0][0].title[:50] + "..." if len(valid_articles[0][0].title) > 50 else valid_articles[0][0].title,
                "source": valid_articles[0][0].source,
                "published": valid_articles[0][0].published,
                "days_old": (now - valid_articles[0][1]).days
            }
        
        return analysis
    
    async def test_single_source_detailed(self, source: str, query: str = "artificial intelligence", days_back: int = 1) -> Dict[str, Any]:
        """è¯¦ç»†æµ‹è¯•å•ä¸ªæœç´¢æº"""
        logger.info(f"è¯¦ç»†æµ‹è¯•æœç´¢æº: {source}")
        
        try:
            # è·å–æœç´¢ç»“æœ
            tool = self.collector.tools.get(source)
            if not tool:
                return {"error": f"æœç´¢æº {source} ä¸å¯ç”¨"}
            
            # æ‰§è¡Œæœç´¢
            articles = await asyncio.to_thread(tool.search, query, days_back)
            
            # åˆ†æç»“æœ
            date_analysis = self.analyze_article_dates(articles, days_back)
            
            return {
                "source": source,
                "articles_found": len(articles),
                "date_analysis": date_analysis,
                "tool_info": {
                    "name": tool.__class__.__name__,
                    "description": getattr(tool, "description", ""),
                    "max_articles": getattr(tool, "max_articles", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æœç´¢æº {source} å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def generate_detailed_report(self, test_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("AIæ–°é—»æ”¶é›†å™¨æ—¶é—´è¿‡æ»¤ä¿®å¤éªŒè¯æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        total_articles = 0
        total_within_range = 0
        total_outside_range = 0
        problematic_sources = []
        
        report.append("1. å„æœç´¢æºè¯¦ç»†æµ‹è¯•ç»“æœ:")
        report.append("-" * 60)
        
        for source, result in test_results.items():
            if "error" in result:
                report.append(f"âŒ {source}: é”™è¯¯ - {result['error']}")
                continue
            
            articles_found = result.get("articles_found", 0)
            date_analysis = result.get("date_analysis", {})
            
            total_articles += articles_found
            total_within_range += date_analysis.get("within_range", 0)
            total_outside_range += date_analysis.get("outside_range", 0)
            
            report.append(f"âœ… {source}: æ‰¾åˆ° {articles_found} ç¯‡æ–‡ç« ")
            
            # æ˜¾ç¤ºæ—¥æœŸåˆ†å¸ƒ
            date_dist = date_analysis.get("date_distribution", {})
            if date_dist:
                report.append(f"   ğŸ“Š æ—¥æœŸåˆ†å¸ƒ:")
                for period, count in date_dist.items():
                    report.append(f"      {period}: {count} ç¯‡")
            
            # æ˜¾ç¤ºæ—¶é—´è¿‡æ»¤æ•ˆæœ
            within_range = date_analysis.get("within_range", 0)
            outside_range = date_analysis.get("outside_range", 0)
            
            if outside_range > 0:
                report.append(f"   âš ï¸  æ—¶é—´è¿‡æ»¤æ•ˆæœ: {within_range} ç¯‡åœ¨èŒƒå›´å†…, {outside_range} ç¯‡è¶…å‡ºèŒƒå›´")
                problematic_sources.append(source)
                
                # æ˜¾ç¤ºæœ€è€çš„æ–‡ç« 
                oldest = date_analysis.get("oldest_article")
                if oldest:
                    report.append(f"   ğŸ“… æœ€è€æ–‡ç« : {oldest['days_old']}å¤©å‰ - {oldest['title']}")
            else:
                report.append(f"   âœ… æ—¶é—´è¿‡æ»¤æ•ˆæœ: æ‰€æœ‰ {within_range} ç¯‡æ–‡ç« éƒ½åœ¨æ—¶é—´èŒƒå›´å†…")
            
            report.append("")
        
        report.append("2. æ€»ä½“ç»Ÿè®¡:")
        report.append("-" * 60)
        report.append(f"æ€»æ–‡ç« æ•°: {total_articles}")
        report.append(f"æ—¶é—´èŒƒå›´å†…: {total_within_range}")
        report.append(f"è¶…å‡ºæ—¶é—´èŒƒå›´: {total_outside_range}")
        
        if total_articles > 0:
            accuracy = (total_within_range / total_articles) * 100
            report.append(f"æ—¶é—´è¿‡æ»¤å‡†ç¡®ç‡: {accuracy:.1f}%")
        
        report.append("")
        
        report.append("3. ä¿®å¤æ•ˆæœè¯„ä¼°:")
        report.append("-" * 60)
        
        if problematic_sources:
            report.append(f"âŒ ä»æœ‰é—®é¢˜çš„æœç´¢æº: {', '.join(problematic_sources)}")
            report.append("   è¿™äº›æœç´¢æºä»ç„¶è¿”å›è¶…å‡ºæ—¶é—´èŒƒå›´çš„æ–‡ç« ")
        else:
            report.append("âœ… æ‰€æœ‰æœç´¢æºçš„æ—¶é—´è¿‡æ»¤éƒ½æ­£å¸¸å·¥ä½œ")
        
        if total_outside_range == 0:
            report.append("ğŸ‰ æ—¶é—´è¿‡æ»¤ä¿®å¤æˆåŠŸï¼æ‰€æœ‰æ–‡ç« éƒ½åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…")
        else:
            report.append(f"âš ï¸  ä»æœ‰ {total_outside_range} ç¯‡æ–‡ç« è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        report.append("")
        
        report.append("4. ä¿®å¤å†…å®¹æ€»ç»“:")
        report.append("-" * 60)
        report.append("âœ… Brave Search API: æ·»åŠ äº† freshness å‚æ•°æ”¯æŒ")
        report.append("âœ… Tavily API: æ·»åŠ äº† time_range å’Œ days å‚æ•°æ”¯æŒ")
        report.append("âœ… Serper API: æ·»åŠ äº†å®¢æˆ·ç«¯æ—¶é—´è¿‡æ»¤")
        report.append("âœ… MetaSota API: ä¼˜åŒ–äº†å®¢æˆ·ç«¯æ—¶é—´è¿‡æ»¤")
        report.append("âœ… æ‰€æœ‰API: éƒ½æ·»åŠ äº†å®¢æˆ·ç«¯æ—¶é—´è¿‡æ»¤ä½œä¸ºå¤‡ç”¨")
        
        return "\n".join(report)

async def main():
    """ä¸»å‡½æ•°"""
    tester = DateFilteringTester()
    
    # æµ‹è¯•æ‰€æœ‰æœç´¢æº
    test_results = {}
    available_sources = tester.collector.get_available_sources()
    
    for source in available_sources:
        result = await tester.test_single_source_detailed(source, "artificial intelligence", days_back=1)
        test_results[source] = result
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report = tester.generate_detailed_report(test_results)
    
    print(report)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    with open("date_filtering_fix_verification_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("\nè¯¦ç»†éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: date_filtering_fix_verification_report.txt")

if __name__ == "__main__":
    asyncio.run(main())
