#!/usr/bin/env python3
"""
æ—¶é—´è¿‡æ»¤é—®é¢˜è¯Šæ–­å·¥å…·
ç”¨äºå®šä½ai_news_collector_libä¸­æ—¶é—´è¿‡æ»¤ä¸å‡†ç¡®çš„é—®é¢˜
"""

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ai_news_collector_lib'))

from ai_news_collector_lib.config.settings import SearchConfig
from ai_news_collector_lib.core.collector import AINewsCollector
from ai_news_collector_lib.models.article import Article

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DateFilteringDiagnostic:
    """æ—¶é—´è¿‡æ»¤è¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.config = SearchConfig()
        self.collector = AINewsCollector(self.config)
        
    def analyze_search_tools(self) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„ä¸ªæœç´¢å·¥å…·çš„æ—¶é—´è¿‡æ»¤å®ç°"""
        analysis = {}
        
        # æ£€æŸ¥å„ä¸ªæœç´¢å·¥å…·çš„æ—¶é—´è¿‡æ»¤å®ç°
        tools_info = {
            "hackernews": {
                "has_date_filter": False,
                "description": "HackerNewså·¥å…·æ²¡æœ‰å®ç°æ—¶é—´è¿‡æ»¤",
                "issue": "HackerNews APIæœ¬èº«ä¸æä¾›æ—¶é—´è¿‡æ»¤ï¼Œåªèƒ½è·å–æœ€æ–°æ–‡ç« "
            },
            "arxiv": {
                "has_date_filter": True,
                "description": "Arxivå·¥å…·ä½¿ç”¨sortBy=submittedDateå’ŒsortOrder=descending",
                "issue": "å¯èƒ½è·å–åˆ°è¾ƒè€çš„æ–‡ç« ï¼Œå› ä¸ºArxivæŒ‰æäº¤æ—¶é—´æ’åºï¼Œä¸æ˜¯å‘å¸ƒæ—¶é—´"
            },
            "duckduckgo": {
                "has_date_filter": True,
                "description": "DuckDuckGoå·¥å…·ä½¿ç”¨after:æ—¥æœŸæ ¼å¼è¿‡æ»¤",
                "issue": "DuckDuckGoçš„æ—¶é—´è¿‡æ»¤å¯èƒ½ä¸å¤Ÿç²¾ç¡®"
            },
            "newsapi": {
                "has_date_filter": True,
                "description": "NewsAPIå·¥å…·ä½¿ç”¨fromå‚æ•°æŒ‡å®šå¼€å§‹æ—¥æœŸ",
                "issue": "NewsAPIçš„æ—¶é—´è¿‡æ»¤ç›¸å¯¹å‡†ç¡®"
            },
            "tavily": {
                "has_date_filter": False,
                "description": "Tavilyå·¥å…·æ²¡æœ‰å®ç°æ—¶é—´è¿‡æ»¤",
                "issue": "Tavily APIè°ƒç”¨ä¸­æ²¡æœ‰ä½¿ç”¨æ—¶é—´å‚æ•°"
            },
            "google_search": {
                "has_date_filter": True,
                "description": "Googleæœç´¢å·¥å…·ä½¿ç”¨dateRestrictå‚æ•°",
                "issue": "Googleçš„dateRestrictå‚æ•°å¯èƒ½ä¸å¤Ÿç²¾ç¡®"
            },
            "serper": {
                "has_date_filter": False,
                "description": "Serperå·¥å…·æ²¡æœ‰å®ç°æ—¶é—´è¿‡æ»¤",
                "issue": "Serper APIè°ƒç”¨ä¸­æ²¡æœ‰ä½¿ç”¨æ—¶é—´å‚æ•°"
            },
            "brave_search": {
                "has_date_filter": False,
                "description": "Braveæœç´¢å·¥å…·æ²¡æœ‰å®ç°æ—¶é—´è¿‡æ»¤",
                "issue": "Brave APIè°ƒç”¨ä¸­æ²¡æœ‰ä½¿ç”¨æ—¶é—´å‚æ•°"
            },
            "metasota_search": {
                "has_date_filter": False,
                "description": "MetaSotaæœç´¢å·¥å…·æ²¡æœ‰å®ç°æ—¶é—´è¿‡æ»¤",
                "issue": "MetaSota MCPè°ƒç”¨ä¸­æ²¡æœ‰ä½¿ç”¨æ—¶é—´å‚æ•°"
            }
        }
        
        return tools_info
    
    def check_article_dates(self, articles: List[Article], days_back: int) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡ç« æ—¥æœŸæ˜¯å¦ç¬¦åˆè¦æ±‚"""
        now = datetime.now(timezone.utc)
        cutoff_date = now - timedelta(days=days_back)
        
        results = {
            "total_articles": len(articles),
            "within_date_range": 0,
            "outside_date_range": 0,
            "invalid_dates": 0,
            "old_articles": [],
            "date_analysis": []
        }
        
        for article in articles:
            try:
                if not article.published:
                    results["invalid_dates"] += 1
                    continue
                
                # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
                published_str = article.published
                if published_str.endswith("Z"):
                    published_str = published_str[:-1] + "+00:00"
                
                published_time = datetime.fromisoformat(published_str)
                
                if published_time >= cutoff_date:
                    results["within_date_range"] += 1
                else:
                    results["outside_date_range"] += 1
                    days_old = (now - published_time).days
                    results["old_articles"].append({
                        "title": article.title[:50] + "..." if len(article.title) > 50 else article.title,
                        "source": article.source,
                        "published": article.published,
                        "days_old": days_old
                    })
                
                results["date_analysis"].append({
                    "source": article.source,
                    "published": article.published,
                    "is_recent": published_time >= cutoff_date,
                    "days_old": (now - published_time).days if published_time < now else 0
                })
                
            except (ValueError, TypeError) as e:
                results["invalid_dates"] += 1
                logger.warning(f"æ— æ³•è§£ææ—¥æœŸ: {article.published} - {e}")
        
        return results
    
    async def test_single_source(self, source: str, query: str = "artificial intelligence", days_back: int = 1) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæœç´¢æº"""
        logger.info(f"æµ‹è¯•æœç´¢æº: {source}")
        
        try:
            # è·å–å•ä¸ªæºçš„æœç´¢ç»“æœ
            tool = self.collector.tools.get(source)
            if not tool:
                return {"error": f"æœç´¢æº {source} ä¸å¯ç”¨"}
            
            # æ‰§è¡Œæœç´¢
            articles = await asyncio.to_thread(tool.search, query, days_back)
            
            # åˆ†æç»“æœ
            date_analysis = self.check_article_dates(articles, days_back)
            
            return {
                "source": source,
                "articles_found": len(articles),
                "date_analysis": date_analysis,
                "tool_info": {
                    "name": tool.__class__.__name__,
                    "description": getattr(tool, "description", ""),
                    "max_articles": getattr(tool, "max_articles", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æœç´¢æº {source} å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def test_all_sources(self, query: str = "artificial intelligence", days_back: int = 1) -> Dict[str, Any]:
        """æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æœç´¢æº"""
        logger.info(f"å¼€å§‹æµ‹è¯•æ‰€æœ‰æœç´¢æºï¼ŒæŸ¥è¯¢: {query}, å¤©æ•°: {days_back}")
        
        available_sources = self.collector.get_available_sources()
        logger.info(f"å¯ç”¨çš„æœç´¢æº: {available_sources}")
        
        results = {}
        
        # å¹¶å‘æµ‹è¯•æ‰€æœ‰æº
        tasks = []
        for source in available_sources:
            task = self.test_single_source(source, query, days_back)
            tasks.append(task)
        
        if tasks:
            test_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(test_results):
                source = available_sources[i]
                if isinstance(result, Exception):
                    results[source] = {"error": str(result)}
                else:
                    results[source] = result
        
        return results
    
    def generate_report(self, test_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("AIæ–°é—»æ”¶é›†å™¨æ—¶é—´è¿‡æ»¤é—®é¢˜è¯Šæ–­æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # åˆ†æå„ä¸ªæœç´¢å·¥å…·çš„å®ç°
        tools_analysis = self.analyze_search_tools()
        
        report.append("1. æœç´¢å·¥å…·æ—¶é—´è¿‡æ»¤å®ç°åˆ†æ:")
        report.append("-" * 50)
        
        problematic_sources = []
        for source, info in tools_analysis.items():
            status = "âœ“" if info["has_date_filter"] else "âœ—"
            report.append(f"{status} {source}: {info['description']}")
            if not info["has_date_filter"]:
                problematic_sources.append(source)
                report.append(f"   é—®é¢˜: {info['issue']}")
        
        report.append("")
        report.append("2. å®é™…æµ‹è¯•ç»“æœ:")
        report.append("-" * 50)
        
        total_articles = 0
        total_old_articles = 0
        
        for source, result in test_results.items():
            if "error" in result:
                report.append(f"âœ— {source}: é”™è¯¯ - {result['error']}")
                continue
            
            articles_found = result.get("articles_found", 0)
            date_analysis = result.get("date_analysis", {})
            
            total_articles += articles_found
            total_old_articles += date_analysis.get("outside_date_range", 0)
            
            report.append(f"âœ“ {source}: æ‰¾åˆ° {articles_found} ç¯‡æ–‡ç« ")
            
            if date_analysis.get("outside_date_range", 0) > 0:
                report.append(f"  âš ï¸  å…¶ä¸­ {date_analysis['outside_date_range']} ç¯‡è¶…å‡ºæ—¶é—´èŒƒå›´")
                
                # æ˜¾ç¤ºæœ€è€çš„æ–‡ç« 
                old_articles = date_analysis.get("old_articles", [])
                if old_articles:
                    oldest = max(old_articles, key=lambda x: x["days_old"])
                    report.append(f"  ğŸ“… æœ€è€æ–‡ç« : {oldest['days_old']}å¤©å‰ - {oldest['title']}")
        
        report.append("")
        report.append("3. é—®é¢˜æ€»ç»“:")
        report.append("-" * 50)
        
        if problematic_sources:
            report.append(f"âŒ ä»¥ä¸‹æœç´¢æºæ²¡æœ‰å®ç°æ—¶é—´è¿‡æ»¤: {', '.join(problematic_sources)}")
        
        if total_old_articles > 0:
            report.append(f"âŒ æ€»å…±å‘ç° {total_old_articles} ç¯‡è¶…å‡ºæ—¶é—´èŒƒå›´çš„æ–‡ç« ")
            report.append("   å»ºè®®æ£€æŸ¥è¿™äº›æœç´¢æºçš„æ—¶é—´è¿‡æ»¤å®ç°")
        
        report.append("")
        report.append("4. ä¿®å¤å»ºè®®:")
        report.append("-" * 50)
        
        for source in problematic_sources:
            if source == "hackernews":
                report.append(f"â€¢ {source}: è€ƒè™‘åœ¨å®¢æˆ·ç«¯è¿›è¡Œæ—¶é—´è¿‡æ»¤")
            elif source in ["tavily", "serper", "brave_search", "metasota_search"]:
                report.append(f"â€¢ {source}: æ£€æŸ¥APIæ–‡æ¡£ï¼Œæ·»åŠ æ—¶é—´è¿‡æ»¤å‚æ•°")
            else:
                report.append(f"â€¢ {source}: æ£€æŸ¥æ—¶é—´è¿‡æ»¤å‚æ•°æ˜¯å¦æ­£ç¡®ä¼ é€’")
        
        report.append("")
        report.append("5. ä»£ç ä½ç½®:")
        report.append("-" * 50)
        report.append("â€¢ æœç´¢å·¥å…·å®ç°: ai_news_collector_lib/tools/search_tools.py")
        report.append("â€¢ æ—¶é—´è¿‡æ»¤é€»è¾‘: BaseSearchTool._filter_by_date()")
        report.append("â€¢ é…ç½®è®¾ç½®: ai_news_collector_lib/config/settings.py")
        
        return "\n".join(report)

async def main():
    """ä¸»å‡½æ•°"""
    diagnostic = DateFilteringDiagnostic()
    
    # æµ‹è¯•æ‰€æœ‰æœç´¢æº
    test_results = await diagnostic.test_all_sources("artificial intelligence", days_back=1)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = diagnostic.generate_report(test_results)
    
    print(report)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    with open("date_filtering_diagnostic_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("\nè¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: date_filtering_diagnostic_report.txt")

if __name__ == "__main__":
    asyncio.run(main())
