# çµæ´»APIä½¿ç”¨ç¤ºä¾‹

æœ¬æ–‡æ¡£å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ–°çš„çµæ´»APIæ¥å£ï¼Œæ”¯æŒå•ä¸ªæœç´¢å¼•æ“é€‰æ‹©å’Œç‹¬ç«‹å‚æ•°è®¾ç½®ã€‚

## ğŸ¯ ä¸»è¦ç‰¹æ€§

- **å•ä¸ªæœç´¢å¼•æ“é€‰æ‹©**ï¼šå¯ä»¥åªä½¿ç”¨æŸä¸ªç‰¹å®šçš„æœç´¢å¼•æ“
- **ç‹¬ç«‹å‚æ•°è®¾ç½®**ï¼šæ¯ä¸ªæœç´¢å¼•æ“å¯ä»¥è®¾ç½®ä¸åŒçš„å‚æ•°
- **é¢„è®¾æ—¶é—´èŒƒå›´**ï¼šæ”¯æŒä¸€å¤©ã€ä¸€å‘¨ã€ä¸€ä¸ªæœˆã€ä¸€å¹´çš„é¢„è®¾é€‰é¡¹
- **è‡ªå®šä¹‰æ—¶é—´èŒƒå›´**ï¼šæ”¯æŒè‡ªå®šä¹‰å¤©æ•°è®¾ç½®
- **åŠ¨æ€é…ç½®æ›´æ–°**ï¼šå¯ä»¥åœ¨è¿è¡Œæ—¶æ›´æ–°æœç´¢å¼•æ“é…ç½®

## ğŸ“š åŸºç¡€ä½¿ç”¨

### 1. ä½¿ç”¨å•ä¸ªæœç´¢å¼•æ“

```python
import asyncio
from ai_news_collector_lib import collect_with_single_engine, TimeRange

async def main():
    # åªä½¿ç”¨HackerNewsæœç´¢ï¼Œæœç´¢æœ€è¿‘ä¸€å‘¨çš„æ–°é—»
    result = await collect_with_single_engine(
        engine_name="hackernews",
        query="artificial intelligence",
        time_range=TimeRange.ONE_WEEK,
        max_articles=10
    )
    
    print(f"æ‰¾åˆ° {result.unique_articles} ç¯‡æ–‡ç« ")
    for article in result.articles[:3]:
        print(f"- {article.title}")

asyncio.run(main())
```

### 2. ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“

```python
import asyncio
from ai_news_collector_lib import collect_with_multiple_engines, TimeRange

async def main():
    # ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“ï¼Œæœç´¢æœ€è¿‘ä¸€å¤©çš„æ–°é—»
    result = await collect_with_multiple_engines(
        engine_names=["hackernews", "arxiv", "duckduckgo"],
        query="machine learning",
        time_range=TimeRange.ONE_DAY,
        max_articles_per_engine=5
    )
    
    print(f"æ€»å…±æ‰¾åˆ° {result.unique_articles} ç¯‡æ–‡ç« ")
    print(f"å»é‡å‰: {result.total_articles} ç¯‡")
    print(f"å»é‡å: {result.unique_articles} ç¯‡")

asyncio.run(main())
```

## âš™ï¸ é«˜çº§é…ç½®

### 1. åˆ›å»ºçµæ´»é…ç½®

```python
from ai_news_collector_lib import FlexibleSearchConfig, TimeRange, FlexibleAINewsCollector

# åˆ›å»ºé…ç½®
config = FlexibleSearchConfig()

# é…ç½®HackerNewsï¼šæœç´¢æœ€è¿‘ä¸€å¤©ï¼Œæœ€å¤š5ç¯‡æ–‡ç« 
config.set_engine_config(
    engine_name="hackernews",
    enabled=True,
    max_articles=5,
    time_range=TimeRange.ONE_DAY
)

# é…ç½®ArXivï¼šæœç´¢æœ€è¿‘ä¸€ä¸ªæœˆï¼Œæœ€å¤š15ç¯‡æ–‡ç« 
config.set_engine_config(
    engine_name="arxiv",
    enabled=True,
    max_articles=15,
    time_range=TimeRange.ONE_MONTH
)

# é…ç½®DuckDuckGoï¼šæœç´¢æœ€è¿‘ä¸€å‘¨ï¼Œæœ€å¤š10ç¯‡æ–‡ç« 
config.set_engine_config(
    engine_name="duckduckgo",
    enabled=True,
    max_articles=10,
    time_range=TimeRange.ONE_WEEK
)

# åˆ›å»ºæ”¶é›†å™¨
collector = FlexibleAINewsCollector(config)
```

### 2. ä½¿ç”¨æ”¶é›†å™¨

```python
import asyncio

async def main():
    # æœç´¢æ‰€æœ‰å¯ç”¨çš„å¼•æ“
    result = await collector.collect_news("deep learning")
    
    # åªæœç´¢ç‰¹å®šå¼•æ“
    result = await collector.collect_news(
        "neural networks", 
        engines=["hackernews", "arxiv"]
    )
    
    print(f"æœç´¢ç»“æœ: {result.unique_articles} ç¯‡æ–‡ç« ")

asyncio.run(main())
```

## ğŸ•’ æ—¶é—´èŒƒå›´è®¾ç½®

### 1. é¢„è®¾æ—¶é—´èŒƒå›´

```python
from ai_news_collector_lib import TimeRange

# ä¸€å¤©å†…çš„æ–°é—»
config.set_engine_config("hackernews", time_range=TimeRange.ONE_DAY)

# ä¸€å‘¨å†…çš„æ–°é—»
config.set_engine_config("arxiv", time_range=TimeRange.ONE_WEEK)

# ä¸€ä¸ªæœˆå†…çš„æ–°é—»
config.set_engine_config("duckduckgo", time_range=TimeRange.ONE_MONTH)

# ä¸€å¹´å†…çš„æ–°é—»
config.set_engine_config("newsapi", time_range=TimeRange.ONE_YEAR)
```

### 2. è‡ªå®šä¹‰æ—¶é—´èŒƒå›´

```python
# è‡ªå®šä¹‰æœç´¢æœ€è¿‘3å¤©çš„æ–°é—»
config.set_engine_config(
    engine_name="hackernews",
    time_range=TimeRange.CUSTOM,
    custom_days=3
)
```

### 3. ä¸ºæ‰€æœ‰å¼•æ“è®¾ç½®ç»Ÿä¸€æ—¶é—´èŒƒå›´

```python
# æ‰€æœ‰å¼•æ“éƒ½æœç´¢æœ€è¿‘ä¸€å‘¨çš„æ–°é—»
config.set_time_range_preset(TimeRange.ONE_WEEK)
```

## ğŸ”§ åŠ¨æ€é…ç½®æ›´æ–°

### 1. è¿è¡Œæ—¶æ›´æ–°å¼•æ“é…ç½®

```python
# å¯ç”¨æ–°çš„æœç´¢å¼•æ“
collector.update_engine_config(
    engine_name="tavily",
    enabled=True,
    api_key="your-api-key",
    max_articles=8,
    time_range=TimeRange.ONE_DAY
)

# æ›´æ–°ç°æœ‰å¼•æ“çš„æ—¶é—´èŒƒå›´
collector.set_time_range_for_engine("hackernews", TimeRange.ONE_MONTH)

# ä¸ºæ‰€æœ‰å¼•æ“è®¾ç½®æ–°çš„æ—¶é—´èŒƒå›´
collector.set_time_range_for_all_engines(TimeRange.ONE_WEEK)
```

### 2. æŸ¥çœ‹å¼•æ“ä¿¡æ¯

```python
# è·å–å¯ç”¨çš„æœç´¢å¼•æ“
available_engines = collector.get_available_engines()
print(f"å¯ç”¨å¼•æ“: {available_engines}")

# è·å–å¼•æ“è¯¦ç»†ä¿¡æ¯
engine_info = collector.get_engine_info()
for engine_name, info in engine_info.items():
    print(f"{engine_name}: {info['time_range']}, {info['max_articles']} ç¯‡æ–‡ç« ")
```

## ğŸ’¡ å®ç”¨ç¤ºä¾‹

### 1. å¿«é€Ÿæœç´¢æœ€è¿‘çƒ­ç‚¹

```python
import asyncio
from ai_news_collector_lib import collect_with_single_engine, TimeRange

async def search_today_hot_news(topic: str):
    """æœç´¢ä»Šå¤©çš„çƒ­ç‚¹æ–°é—»"""
    result = await collect_with_single_engine(
        engine_name="hackernews",
        query=topic,
        time_range=TimeRange.ONE_DAY,
        max_articles=20
    )
    
    print(f"ä»Šå¤©å…³äº '{topic}' çš„çƒ­ç‚¹æ–°é—»:")
    for i, article in enumerate(result.articles, 1):
        print(f"{i}. {article.title}")
        print(f"   æ¥æº: {article.source_name}")
        print(f"   æ—¶é—´: {article.published}")
        print()

asyncio.run(search_today_hot_news("AI"))
```

### 2. å­¦æœ¯è®ºæ–‡æœç´¢

```python
import asyncio
from ai_news_collector_lib import collect_with_single_engine, TimeRange

async def search_recent_papers(topic: str):
    """æœç´¢æœ€è¿‘çš„å­¦æœ¯è®ºæ–‡"""
    result = await collect_with_single_engine(
        engine_name="arxiv",
        query=topic,
        time_range=TimeRange.ONE_MONTH,
        max_articles=15
    )
    
    print(f"æœ€è¿‘ä¸€ä¸ªæœˆå…³äº '{topic}' çš„è®ºæ–‡:")
    for article in result.articles:
        print(f"- {article.title}")
        print(f"  æ‘˜è¦: {article.summary[:100]}...")
        print(f"  é“¾æ¥: {article.url}")
        print()

asyncio.run(search_recent_papers("transformer"))
```

### 3. å¤šæºå¯¹æ¯”æœç´¢

```python
import asyncio
from ai_news_collector_lib import FlexibleSearchConfig, TimeRange, FlexibleAINewsCollector

async def compare_sources(topic: str):
    """å¯¹æ¯”ä¸åŒæœç´¢å¼•æ“çš„ç»“æœ"""
    config = FlexibleSearchConfig()
    
    # é…ç½®ä¸åŒå¼•æ“ä½¿ç”¨ä¸åŒçš„æ—¶é—´èŒƒå›´
    config.set_engine_config("hackernews", time_range=TimeRange.ONE_DAY, max_articles=5)
    config.set_engine_config("arxiv", time_range=TimeRange.ONE_WEEK, max_articles=5)
    config.set_engine_config("duckduckgo", time_range=TimeRange.ONE_MONTH, max_articles=5)
    
    collector = FlexibleAINewsCollector(config)
    result = await collector.collect_news(topic)
    
    # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤ºç»“æœ
    by_source = {}
    for article in result.articles:
        source = article.source
        if source not in by_source:
            by_source[source] = []
        by_source[source].append(article)
    
    for source, articles in by_source.items():
        print(f"\n=== {source.upper()} ({len(articles)} ç¯‡) ===")
        for article in articles:
            print(f"- {article.title}")

asyncio.run(compare_sources("quantum computing"))
```

### 4. æ¸è¿›å¼æœç´¢

```python
import asyncio
from ai_news_collector_lib import FlexibleSearchConfig, TimeRange, FlexibleAINewsCollector

async def progressive_search(topic: str):
    """æ¸è¿›å¼æœç´¢ï¼šä»æœ€è¿‘åˆ°æ›´æ—©çš„æ—¶é—´"""
    config = FlexibleSearchConfig()
    
    # å…ˆæœç´¢æœ€è¿‘ä¸€å¤©
    config.set_engine_config("hackernews", time_range=TimeRange.ONE_DAY, max_articles=10)
    collector = FlexibleAINewsCollector(config)
    
    print("=== æœ€è¿‘ä¸€å¤©çš„æ–°é—» ===")
    result1 = await collector.collect_news(topic)
    print(f"æ‰¾åˆ° {result1.unique_articles} ç¯‡æ–‡ç« ")
    
    # å¦‚æœç»“æœä¸å¤Ÿï¼Œæ‰©å±•åˆ°æœ€è¿‘ä¸€å‘¨
    if result1.unique_articles < 5:
        print("\n=== æ‰©å±•åˆ°æœ€è¿‘ä¸€å‘¨ ===")
        collector.set_time_range_for_engine("hackernews", TimeRange.ONE_WEEK)
        result2 = await collector.collect_news(topic)
        print(f"æ‰¾åˆ° {result2.unique_articles} ç¯‡æ–‡ç« ")
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œå†æ‰©å±•åˆ°æœ€è¿‘ä¸€ä¸ªæœˆ
        if result2.unique_articles < 10:
            print("\n=== æ‰©å±•åˆ°æœ€è¿‘ä¸€ä¸ªæœˆ ===")
            collector.set_time_range_for_engine("hackernews", TimeRange.ONE_MONTH)
            result3 = await collector.collect_news(topic)
            print(f"æ‰¾åˆ° {result3.unique_articles} ç¯‡æ–‡ç« ")

asyncio.run(progressive_search("blockchain"))
```

## ğŸ” æ”¯æŒçš„æœç´¢å¼•æ“

### å…è´¹æœç´¢å¼•æ“
- `hackernews` - HackerNewsæŠ€æœ¯ç¤¾åŒº
- `arxiv` - å­¦æœ¯è®ºæ–‡é¢„å°æœ¬
- `duckduckgo` - éšç§æœç´¢å¼•æ“
- `rss_feeds` - RSSè®¢é˜…æº

### ä»˜è´¹æœç´¢å¼•æ“ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
- `newsapi` - NewsAPIæ–°é—»èšåˆ
- `tavily` - Tavily AIæœç´¢
- `google_search` - Googleæœç´¢API
- `bing_search` - Bingæœç´¢API
- `serper` - Serperæœç´¢API
- `brave_search` - Braveæœç´¢API
- `metasota_search` - MetaSotaæœç´¢API

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥**ï¼šä»˜è´¹æœç´¢å¼•æ“éœ€è¦ç›¸åº”çš„APIå¯†é’¥ï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–ç›´æ¥ä¼ å…¥
2. **æ—¶é—´èŒƒå›´**ï¼šä¸åŒæœç´¢å¼•æ“å¯¹æ—¶é—´è¿‡æ»¤çš„æ”¯æŒç¨‹åº¦ä¸åŒï¼Œå»ºè®®ç»“åˆå®¢æˆ·ç«¯è¿‡æ»¤
3. **å¹¶å‘é™åˆ¶**ï¼šæŸäº›APIå¯èƒ½æœ‰å¹¶å‘è¯·æ±‚é™åˆ¶ï¼Œå»ºè®®åˆç†è®¾ç½®å¹¶å‘æ•°é‡
4. **é”™è¯¯å¤„ç†**ï¼šå»ºè®®æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†ï¼Œç‰¹åˆ«æ˜¯ç½‘ç»œè¯·æ±‚å¤±è´¥çš„æƒ…å†µ

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **åˆç†è®¾ç½®æ–‡ç« æ•°é‡**ï¼šæ ¹æ®éœ€æ±‚è®¾ç½®åˆé€‚çš„`max_articles`å€¼
2. **é€‰æ‹©åˆé€‚çš„æ—¶é—´èŒƒå›´**ï¼šè¾ƒçŸ­çš„æ—¶é—´èŒƒå›´é€šå¸¸è¿”å›æ›´ç›¸å…³çš„ç»“æœ
3. **ä½¿ç”¨ç¼“å­˜**ï¼šå¯¹äºé‡å¤æŸ¥è¯¢ï¼Œå»ºè®®å¯ç”¨ç¼“å­˜åŠŸèƒ½
4. **å¹¶å‘æ§åˆ¶**ï¼šé¿å…åŒæ—¶å¯åŠ¨è¿‡å¤šæœç´¢å¼•æ“ï¼Œä»¥å…è§¦å‘APIé™åˆ¶
