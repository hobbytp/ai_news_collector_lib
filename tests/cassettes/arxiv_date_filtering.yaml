interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: http://export.arxiv.org/api/query?max_results=5&search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+AND+all%3Amachine+learning&sortBy=submittedDate&sortOrder=descending&start=0
  response:
    body:
      string: ''
    headers:
      Accept-Ranges:
      - bytes
      Connection:
      - close
      Content-Length:
      - '0'
      Date:
      - Thu, 30 Oct 2025 01:50:00 GMT
      Location:
      - https://export.arxiv.org/api/query?search_query=cat:cs.AI%20OR%20cat:cs.LG%20OR%20cat:cs.CL%20AND%20all:machine%20learning&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending
      Retry-After:
      - '0'
      Server:
      - Varnish
      Strict-Transport-Security:
      - max-age=300
      Via:
      - 1.1 varnish
      X-Cache:
      - HIT
      X-Cache-Hits:
      - '0'
      X-Served-By:
      - cache-itm1220028-ITM
      X-Timer:
      - S1761789001.562345,VS0,VE0
    status:
      code: 301
      message: Moved Permanently
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: https://localhost:33210/api/query?max_results=5&search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+AND+all%3Amachine+learning&sortBy=submittedDate&sortOrder=descending&start=0
  response:
    body:
      string: "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n
        \ <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%20OR%20cat%3Acs.LG%20OR%20cat%3Acs.CL%20AND%20all%3Amachine%20learning%26id_list%3D%26start%3D0%26max_results%3D5\"
        rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv
        Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL AND all:machine learning&amp;id_list=&amp;start=0&amp;max_results=5</title>\n
        \ <id>http://arxiv.org/api/D9jbOwMlxm13ZRJT6OVevFlaMRw</id>\n  <updated>2025-10-29T00:00:00-04:00</updated>\n
        \ <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">33920</opensearch:totalResults>\n
        \ <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n
        \ <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">5</opensearch:itemsPerPage>\n
        \ <entry>\n    <id>http://arxiv.org/abs/2510.24701v1</id>\n    <updated>2025-10-28T17:53:02Z</updated>\n
        \   <published>2025-10-28T17:53:02Z</published>\n    <title>Tongyi DeepResearch
        Technical Report</title>\n    <summary>  We present Tongyi DeepResearch, an
        agentic large language model, which is\nspecifically designed for long-horizon,
        deep information-seeking research\ntasks. To incentivize autonomous deep research
        agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework
        that combines agentic\nmid-training and agentic post-training, enabling scalable
        reasoning and\ninformation seeking across complex tasks. We design a highly
        scalable data\nsynthesis pipeline that is fully automatic, without relying
        on costly human\nannotation, and empowers all training stages. By constructing
        customized\nenvironments for each stage, our system enables stable and consistent\ninteractions
        throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters,
        with only 3.3 billion activated per token, achieves\nstate-of-the-art performance
        across a range of agentic deep research\nbenchmarks, including Humanity's
        Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES
        and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete
        solutions to empower the\ncommunity.\n</summary>\n    <author>\n      <name>
        Tongyi DeepResearch Team</name>\n    </author>\n    <author>\n      <name>Baixuan
        Li</name>\n    </author>\n    <author>\n      <name>Bo Zhang</name>\n    </author>\n
        \   <author>\n      <name>Dingchu Zhang</name>\n    </author>\n    <author>\n
        \     <name>Fei Huang</name>\n    </author>\n    <author>\n      <name>Guangyu
        Li</name>\n    </author>\n    <author>\n      <name>Guoxin Chen</name>\n    </author>\n
        \   <author>\n      <name>Huifeng Yin</name>\n    </author>\n    <author>\n
        \     <name>Jialong Wu</name>\n    </author>\n    <author>\n      <name>Jingren
        Zhou</name>\n    </author>\n    <author>\n      <name>Kuan Li</name>\n    </author>\n
        \   <author>\n      <name>Liangcai Su</name>\n    </author>\n    <author>\n
        \     <name>Litu Ou</name>\n    </author>\n    <author>\n      <name>Liwen
        Zhang</name>\n    </author>\n    <author>\n      <name>Pengjun Xie</name>\n
        \   </author>\n    <author>\n      <name>Rui Ye</name>\n    </author>\n    <author>\n
        \     <name>Wenbiao Yin</name>\n    </author>\n    <author>\n      <name>Xinmiao
        Yu</name>\n    </author>\n    <author>\n      <name>Xinyu Wang</name>\n    </author>\n
        \   <author>\n      <name>Xixi Wu</name>\n    </author>\n    <author>\n      <name>Xuanzhong
        Chen</name>\n    </author>\n    <author>\n      <name>Yida Zhao</name>\n    </author>\n
        \   <author>\n      <name>Zhen Zhang</name>\n    </author>\n    <author>\n
        \     <name>Zhengwei Tao</name>\n    </author>\n    <author>\n      <name>Zhongwang
        Zhang</name>\n    </author>\n    <author>\n      <name>Zile Qiao</name>\n
        \   </author>\n    <author>\n      <name>Chenxi Wang</name>\n    </author>\n
        \   <author>\n      <name>Donglei Yu</name>\n    </author>\n    <author>\n
        \     <name>Gang Fu</name>\n    </author>\n    <author>\n      <name>Haiyang
        Shen</name>\n    </author>\n    <author>\n      <name>Jiayin Yang</name>\n
        \   </author>\n    <author>\n      <name>Jun Lin</name>\n    </author>\n    <author>\n
        \     <name>Junkai Zhang</name>\n    </author>\n    <author>\n      <name>Kui
        Zeng</name>\n    </author>\n    <author>\n      <name>Li Yang</name>\n    </author>\n
        \   <author>\n      <name>Hailong Yin</name>\n    </author>\n    <author>\n
        \     <name>Maojia Song</name>\n    </author>\n    <author>\n      <name>Ming
        Yan</name>\n    </author>\n    <author>\n      <name>Peng Xia</name>\n    </author>\n
        \   <author>\n      <name>Qian Xiao</name>\n    </author>\n    <author>\n
        \     <name>Rui Min</name>\n    </author>\n    <author>\n      <name>Ruixue
        Ding</name>\n    </author>\n    <author>\n      <name>Runnan Fang</name>\n
        \   </author>\n    <author>\n      <name>Shaowei Chen</name>\n    </author>\n
        \   <author>\n      <name>Shen Huang</name>\n    </author>\n    <author>\n
        \     <name>Shihang Wang</name>\n    </author>\n    <author>\n      <name>Shihao
        Cai</name>\n    </author>\n    <author>\n      <name>Weizhou Shen</name>\n
        \   </author>\n    <author>\n      <name>Xiaobin Wang</name>\n    </author>\n
        \   <author>\n      <name>Xin Guan</name>\n    </author>\n    <author>\n      <name>Xinyu
        Geng</name>\n    </author>\n    <author>\n      <name>Yingcheng Shi</name>\n
        \   </author>\n    <author>\n      <name>Yuning Wu</name>\n    </author>\n
        \   <author>\n      <name>Zhuo Chen</name>\n    </author>\n    <author>\n
        \     <name>Zijian Li</name>\n    </author>\n    <author>\n      <name>Yong
        Jiang</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">https://tongyi-agent.github.io/blog</arxiv:comment>\n
        \   <link href=\"http://arxiv.org/abs/2510.24701v1\" rel=\"alternate\" type=\"text/html\"/>\n
        \   <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.24701v1\" rel=\"related\"
        type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24699v1</id>\n    <updated>2025-10-28T17:51:50Z</updated>\n
        \   <published>2025-10-28T17:51:50Z</published>\n    <title>AgentFold: Long-Horizon
        Web Agents with Proactive Context Management</title>\n    <summary>  LLM-based
        web agents show immense promise for information seeking, yet their\neffectiveness
        on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management.
        Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate
        noisy, raw histories, while methods that fixedly\nsummarize the full history
        at each step risk the irreversible loss of critical\ndetails. Addressing these,
        we introduce AgentFold, a novel agent paradigm\ncentered on proactive context
        management, inspired by the human cognitive\nprocess of retrospective consolidation.
        AgentFold treats its context as a\ndynamic cognitive workspace to be actively
        sculpted, rather than a passive log\nto be filled. At each step, it learns
        to execute a `folding' operation, which\nmanages its historical trajectory
        at multiple scales: it can perform granular\ncondensations to preserve vital,
        fine-grained details, or deep consolidations\nto abstract away entire multi-step
        sub-tasks. The results on prominent\nbenchmarks are striking: with simple
        supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B
        agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this
        performance not only surpasses or\nmatches open-source models of a dramatically
        larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading
        proprietary agents like\nOpenAI's o4-mini.\n</summary>\n    <author>\n      <name>Rui
        Ye</name>\n    </author>\n    <author>\n      <name>Zhongwang Zhang</name>\n
        \   </author>\n    <author>\n      <name>Kuan Li</name>\n    </author>\n    <author>\n
        \     <name>Huifeng Yin</name>\n    </author>\n    <author>\n      <name>Zhengwei
        Tao</name>\n    </author>\n    <author>\n      <name>Yida Zhao</name>\n    </author>\n
        \   <author>\n      <name>Liangcai Su</name>\n    </author>\n    <author>\n
        \     <name>Liwen Zhang</name>\n    </author>\n    <author>\n      <name>Zile
        Qiao</name>\n    </author>\n    <author>\n      <name>Xinyu Wang</name>\n
        \   </author>\n    <author>\n      <name>Pengjun Xie</name>\n    </author>\n
        \   <author>\n      <name>Fei Huang</name>\n    </author>\n    <author>\n
        \     <name>Siheng Chen</name>\n    </author>\n    <author>\n      <name>Jingren
        Zhou</name>\n    </author>\n    <author>\n      <name>Yong Jiang</name>\n
        \   </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26
        pages, 9 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2510.24699v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.24699v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24664v1</id>\n    <updated>2025-10-28T17:29:59Z</updated>\n
        \   <published>2025-10-28T17:29:59Z</published>\n    <title>MQM Re-Annotation:
        A Technique for Collaborative Evaluation of Machine\n  Translation</title>\n
        \   <summary>  Human evaluation of machine translation is in an arms race
        with translation\nmodel quality: as our models get better, our evaluation
        methods need to be\nimproved to ensure that quality gains are not lost in
        evaluation noise. To this\nend, we experiment with a two-stage version of
        the current state-of-the-art\ntranslation evaluation paradigm (MQM), which
        we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits
        a set of pre-existing MQM\nannotations, that may have come from themselves,
        another human annotator, or an\nautomatic MQM annotation system. We demonstrate
        that rater behavior in\nre-annotation aligns with our goals, and that re-annotation
        results in\nhigher-quality annotations, mostly due to finding errors that
        were missed\nduring the first pass.\n</summary>\n    <author>\n      <name>Parker
        Riley</name>\n    </author>\n    <author>\n      <name>Daniel Deutsch</name>\n
        \   </author>\n    <author>\n      <name>Mara Finkelstein</name>\n    </author>\n
        \   <author>\n      <name>Colten DiIanni</name>\n    </author>\n    <author>\n
        \     <name>Juraj Juraska</name>\n    </author>\n    <author>\n      <name>Markus
        Freitag</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2510.24664v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.24664v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24619v1</id>\n
        \   <updated>2025-10-28T16:48:03Z</updated>\n    <published>2025-10-28T16:48:03Z</published>\n
        \   <title>Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation</title>\n
        \   <summary>  With the release of new large language models (LLMs) like Llama
        and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible
        due to their\nmultilingual pretraining and strong generalization capabilities.
        However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging.
        While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation
        (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning,
        prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot
        transfer in decoder-only models. We present a\ncomprehensive study of three
        prefix-based methods for zero-shot cross-lingual\ntransfer from English to
        35+ high- and low-resource languages. Our analysis\nfurther explores transfer
        across linguistic families and scripts, as well as\nthe impact of scaling
        model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform
        LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements
        were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning
        parameters with prefix tuning, we achieve consistent\nimprovements across
        diverse benchmarks. These findings highlight the potential\nof prefix-based
        techniques as an effective and scalable alternative to LoRA,\nparticularly
        in low-resource multilingual settings.\n</summary>\n    <author>\n      <name>Snegha
        A</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Indian
        Institute of Technology Bombay</arxiv:affiliation>\n    </author>\n    <author>\n
        \     <name>Sayambhu Sen</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Amazon
        Alexa</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Piyush
        Singh Pasi</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Amazon
        Alexa</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Abhishek
        Singhania</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Amazon
        Alexa</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Preethi
        Jyothi</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Indian
        Institute of Technology Bombay</arxiv:affiliation>\n    </author>\n    <arxiv:comment
        xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 Pages</arxiv:comment>\n    <link
        href=\"http://arxiv.org/abs/2510.24619v1\" rel=\"alternate\" type=\"text/html\"/>\n
        \   <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.24619v1\" rel=\"related\"
        type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24592v1</id>\n    <updated>2025-10-28T16:22:54Z</updated>\n
        \   <published>2025-10-28T16:22:54Z</published>\n    <title>ReForm: Reflective
        Autoformalization with Prospective Bounded Sequence\n  Optimization</title>\n
        \   <summary>  Autoformalization, which translates natural language mathematics
        into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning
        to solve math problems stated in natural language. While Large\nLanguage Models
        can generate syntactically correct formal statements, they\noften fail to
        preserve the original problem's semantic intent. This limitation\narises from
        the LLM approaches' treating autoformalization as a simplistic\ntranslation
        task which lacks mechanisms for self-reflection and iterative\nrefinement
        that human experts naturally employ. To address these issues, we\npropose
        ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic
        consistency evaluation into the autoformalization process. This\nenables the
        model to iteratively generate formal statements, assess its\nsemantic fidelity,
        and self-correct identified errors through progressive\nrefinement. To effectively
        train this reflective model, we introduce\nProspective Bounded Sequence Optimization
        (PBSO), which employs different\nrewards at different sequence positions to
        ensure that the model develops both\naccurate autoformalization and correct
        semantic validations, preventing\nsuperficial critiques that would undermine
        the purpose of reflection. Extensive\nexperiments across four autoformalization
        benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2
        percentage points over the strongest\nbaselines. To further ensure evaluation
        reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated
        items that not only\nvalidates LLMs as judges but also reveals that autoformalization
        is inherently\ndifficult: even human experts produce semantic errors in up
        to 38.5% of cases.\n</summary>\n    <author>\n      <name>Guoxin Chen</name>\n
        \   </author>\n    <author>\n      <name>Jing Wu</name>\n    </author>\n    <author>\n
        \     <name>Xinjie Chen</name>\n    </author>\n    <author>\n      <name>Wayne
        Xin Zhao</name>\n    </author>\n    <author>\n      <name>Ruihua Song</name>\n
        \   </author>\n    <author>\n      <name>Chengxi Li</name>\n    </author>\n
        \   <author>\n      <name>Kai Fan</name>\n    </author>\n    <author>\n      <name>Dayiheng
        Liu</name>\n    </author>\n    <author>\n      <name>Minpeng Liao</name>\n
        \   </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ongoing
        Work</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2510.24592v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.24592v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"
    headers:
      Accept-Ranges:
      - bytes
      Age:
      - '0'
      Connection:
      - keep-alive
      Date:
      - Thu, 30 Oct 2025 01:50:03 GMT
      Strict-Transport-Security:
      - max-age=300
      Vary:
      - Accept-Encoding
      X-Cache:
      - MISS, MISS, MISS
      X-Cache-Hits:
      - 0, 0, 0
      X-Served-By:
      - cache-lga21951-LGA, cache-lga21922-LGA, cache-bur-kbur8200121-BUR
      X-Timer:
      - S1761789003.307702,VS0,VE252
      access-control-allow-origin:
      - '*'
      content-length:
      - '17325'
      content-type:
      - application/atom+xml; charset=UTF-8
      server:
      - Apache
      via:
      - 1.1 varnish, 1.1 varnish, 1.1 varnish
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: http://export.arxiv.org/api/query?max_results=5&search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+AND+all%3Amachine+learning&sortBy=submittedDate&sortOrder=descending&start=0
  response:
    body:
      string: ''
    headers:
      Accept-Ranges:
      - bytes
      Connection:
      - close
      Content-Length:
      - '0'
      Date:
      - Thu, 30 Oct 2025 10:52:33 GMT
      Location:
      - https://export.arxiv.org/api/query?search_query=cat:cs.AI%20OR%20cat:cs.LG%20OR%20cat:cs.CL%20AND%20all:machine%20learning&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending
      Retry-After:
      - '0'
      Server:
      - Varnish
      Strict-Transport-Security:
      - max-age=300
      Via:
      - 1.1 varnish
      X-Cache:
      - HIT
      X-Cache-Hits:
      - '0'
      X-Served-By:
      - cache-itm1220072-ITM
      X-Timer:
      - S1761821554.857340,VS0,VE0
    status:
      code: 301
      message: Moved Permanently
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: https://localhost:33210/api/query?max_results=5&search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+AND+all%3Amachine+learning&sortBy=submittedDate&sortOrder=descending&start=0
  response:
    body:
      string: "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n
        \ <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%20OR%20cat%3Acs.LG%20OR%20cat%3Acs.CL%20AND%20all%3Amachine%20learning%26id_list%3D%26start%3D0%26max_results%3D5\"
        rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv
        Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL AND all:machine learning&amp;id_list=&amp;start=0&amp;max_results=5</title>\n
        \ <id>http://arxiv.org/api/D9jbOwMlxm13ZRJT6OVevFlaMRw</id>\n  <updated>2025-10-30T00:00:00-04:00</updated>\n
        \ <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">33936</opensearch:totalResults>\n
        \ <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n
        \ <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">5</opensearch:itemsPerPage>\n
        \ <entry>\n    <id>http://arxiv.org/abs/2510.25701v1</id>\n    <updated>2025-10-29T17:05:00Z</updated>\n
        \   <published>2025-10-29T17:05:00Z</published>\n    <title>Interpreting LLMs
        as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical
        ML?</title>\n    <summary>  Large Language Models (LLMs) are increasingly
        explored as flexible\nalternatives to classical machine learning models for
        classification tasks\nthrough zero-shot prompting. However, their suitability
        for structured tabular\ndata remains underexplored, especially in high-stakes
        financial applications\nsuch as financial risk assessment. This study conducts
        a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM,
        a state-of-the-art\ngradient-boosting model, on a real-world loan default
        prediction task. We\nevaluate their predictive performance, analyze feature
        attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations.
        While LLMs are\nable to identify key financial risk indicators, their feature
        importance\nrankings diverge notably from LightGBM, and their self-explanations
        often fail\nto align with empirical SHAP attributions. These findings highlight
        the\nlimitations of LLMs as standalone models for structured financial risk\nprediction
        and raise concerns about the trustworthiness of their self-generated\nexplanations.
        Our results underscore the need for explainability audits,\nbaseline comparisons
        with interpretable models, and human-in-the-loop oversight\nwhen deploying
        LLMs in risk-sensitive financial environments.\n</summary>\n    <author>\n
        \     <name>Saeed AlMarri</name>\n    </author>\n    <author>\n      <name>Kristof
        Juhasz</name>\n    </author>\n    <author>\n      <name>Mathieu Ravaut</name>\n
        \   </author>\n    <author>\n      <name>Gautier Marti</name>\n    </author>\n
        \   <author>\n      <name>Hamdan Al Ahbabi</name>\n    </author>\n    <author>\n
        \     <name>Ibrahim Elfadel</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8
        pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop</arxiv:comment>\n    <link
        href=\"http://arxiv.org/abs/2510.25701v1\" rel=\"alternate\" type=\"text/html\"/>\n
        \   <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25701v1\" rel=\"related\"
        type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25626v1</id>\n
        \   <updated>2025-10-29T15:30:31Z</updated>\n    <published>2025-10-29T15:30:31Z</published>\n
        \   <title>Are Language Models Efficient Reasoners? A Perspective from Logic\n
        \ Programming</title>\n    <summary>  Modern language models (LMs) exhibit
        strong deductive reasoning capabilities,\nyet standard evaluations emphasize
        correctness while overlooking a key aspect\nof human-like reasoning: efficiency.
        In real-world reasoning scenarios, much of\nthe available information is irrelevant,
        and effective deductive inference\nrequires identifying and ignoring such
        distractions. We propose a framework for\nassessing LM reasoning efficiency
        through the lens of logic programming,\nintroducing a simple method to align
        proofs written in natural language -- as\ngenerated by an LM -- with shortest
        proofs found by executing the logic\nprogram. Efficiency is quantified by
        measuring how well a model avoids\nunnecessary inference. Empirically, we
        construct a dataset of math word\nproblems injected with various number of
        irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We
        find that current LMs show marked\naccuracy declines under such conditions
        -- even with minimal, domain-consistent\ndistractions -- and the proofs they
        generate frequently exhibit detours through\nirrelevant inferences.\n</summary>\n
        \   <author>\n      <name>Andreas Opedal</name>\n    </author>\n    <author>\n
        \     <name>Yanick Zengaffinen</name>\n    </author>\n    <author>\n      <name>Haruki
        Shirakami</name>\n    </author>\n    <author>\n      <name>Clemente Pasti</name>\n
        \   </author>\n    <author>\n      <name>Mrinmaya Sachan</name>\n    </author>\n
        \   <author>\n      <name>Abulhair Saparov</name>\n    </author>\n    <author>\n
        \     <name>Ryan Cotterell</name>\n    </author>\n    <author>\n      <name>Bernhard
        Sch\xF6lkopf</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted
        to NeurIPS 2025</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2510.25626v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25626v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25557v1</id>\n    <updated>2025-10-29T14:21:49Z</updated>\n
        \   <published>2025-10-29T14:21:49Z</published>\n    <title>Hybrid Quantum-Classical
        Recurrent Neural Networks</title>\n    <summary>  We present a hybrid quantum-classical
        recurrent neural network (QRNN)\narchitecture in which the entire recurrent
        core is realized as a parametrized\nquantum circuit (PQC) controlled by a
        classical feedforward network. The hidden\nstate is the quantum state of an
        $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$.
        The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving
        without external constraints.\nAt each timestep, mid-circuit readouts are
        combined with the input embedding\nand processed by the feedforward network,
        which provides explicit classical\nnonlinearity. The outputs parametrize the
        PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact
        and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity
        memory, (ii) partial\nobservation via mid-circuit measurements, and (iii)
        nonlinear classical control\nfor input-conditioned parametrization. We evaluate
        the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST,
        permuted MNIST, copying memory,\nand language modeling, adopting projective
        measurements as a limiting case to\nobtain mid-circuit readouts while maintaining
        a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism
        over the mid-circuit\nreadouts in a sequence-to-sequence model and show its
        effectiveness for machine\ntranslation. To our knowledge, this is the first
        model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive
        performance against\nstrong classical baselines across a broad class of sequence-learning
        tasks.\n</summary>\n    <author>\n      <name>Wenduan Xu</name>\n    </author>\n
        \   <link href=\"http://arxiv.org/abs/2510.25557v1\" rel=\"alternate\" type=\"text/html\"/>\n
        \   <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25557v1\" rel=\"related\"
        type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25232v1</id>\n    <updated>2025-10-29T07:18:43Z</updated>\n
        \   <published>2025-10-29T07:18:43Z</published>\n    <title>From Medical Records
        to Diagnostic Dialogues: A Clinical-Grounded\n  Approach and Dataset for Psychiatric
        Comorbidity</title>\n    <summary>  Psychiatric comorbidity is clinically
        significant yet challenging due to the\ncomplexity of multiple co-occurring
        disorders. To address this, we develop a\nnovel approach integrating synthetic
        patient electronic medical record (EMR)\nconstruction and multi-agent diagnostic
        dialogue generation. We create 502\nsynthetic EMRs for common comorbid conditions
        using a pipeline that ensures\nclinical relevance and diversity. Our multi-agent
        framework transfers the\nclinical interview protocol into a hierarchical state
        machine and context tree,\nsupporting over 130 diagnostic states while maintaining
        clinical standards.\nThrough this rigorous process, we construct PsyCoTalk,
        the first large-scale\ndialogue dataset supporting comorbidity, containing
        3,000 multi-turn diagnostic\ndialogues validated by psychiatrists. This dataset
        enhances diagnostic accuracy\nand treatment planning, offering a valuable
        resource for psychiatric\ncomorbidity research. Compared to real-world clinical
        transcripts, PsyCoTalk\nexhibits high structural and linguistic fidelity in
        terms of dialogue length,\ntoken distribution, and diagnostic reasoning strategies.
        Licensed psychiatrists\nconfirm the realism and diagnostic validity of the
        dialogues. This dataset\nenables the development and evaluation of models
        capable of multi-disorder\npsychiatric screening in a single conversational
        pass.\n</summary>\n    <author>\n      <name>Tianxi Wan</name>\n    </author>\n
        \   <author>\n      <name>Jiaming Luo</name>\n    </author>\n    <author>\n
        \     <name>Siyuan Chen</name>\n    </author>\n    <author>\n      <name>Kunyao
        Lan</name>\n    </author>\n    <author>\n      <name>Jianhua Chen</name>\n
        \   </author>\n    <author>\n      <name>Haiyang Geng</name>\n    </author>\n
        \   <author>\n      <name>Mengyue Wu</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2510.25232v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25232v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25206v1</id>\n    <updated>2025-10-29T06:18:37Z</updated>\n
        \   <published>2025-10-29T06:18:37Z</published>\n    <title>RAVR: Reference-Answer-guided
        Variational Reasoning for Large Language\n  Models</title>\n    <summary>
        \ Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage
        models (LLMs), but critically depends on a key prerequisite: the LLM\ncan
        already generate high-utility reasoning paths with non-negligible\nprobability.
        For tasks beyond the LLM's current competence, such reasoning path\ncan be
        hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning.
        We are motivated by the insight from cognitive science that Why is\nthis the
        answer is often an easier question than What is the answer, as it\navoids
        the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory
        reconstruction-systematically retracing the reasoning that links a\nquestion
        to its answer. We show that LLMs can similarly leverage answers to\nderive
        high-quality reasoning paths. We formalize this phenomenon and prove\nthat
        conditioning on answer provably increases the expected utility of sampled\nreasoning
        paths, thereby transforming intractable problems into learnable ones.\nBuilding
        on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning),
        an end-to-end framework that uses answer-conditioned\nreasoning as a variational
        surrogate for question-only reasoning. Experiments\nin both general and math
        domains demonstrate consistent improvements over\nstrong baselines. We further
        analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens
        conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.\n</summary>\n
        \   <author>\n      <name>Tianqianjin Lin</name>\n    </author>\n    <author>\n
        \     <name>Xi Zhao</name>\n    </author>\n    <author>\n      <name>Xingyao
        Zhang</name>\n    </author>\n    <author>\n      <name>Rujiao Long</name>\n
        \   </author>\n    <author>\n      <name>Yi Xu</name>\n    </author>\n    <author>\n
        \     <name>Zhuoren Jiang</name>\n    </author>\n    <author>\n      <name>Wenbo
        Su</name>\n    </author>\n    <author>\n      <name>Bo Zheng</name>\n    </author>\n
        \   <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages,
        11 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2510.25206v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25206v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n</feed>\n"
    headers:
      Accept-Ranges:
      - bytes
      Age:
      - '0'
      Connection:
      - keep-alive
      Date:
      - Thu, 30 Oct 2025 10:52:37 GMT
      Strict-Transport-Security:
      - max-age=300
      Vary:
      - Accept-Encoding
      X-Cache:
      - MISS, MISS, MISS
      X-Cache-Hits:
      - 0, 0, 0
      X-Served-By:
      - cache-lga21937-LGA, cache-lga21922-LGA, cache-bur-kbur8200114-BUR
      X-Timer:
      - S1761821557.847186,VS0,VE223
      access-control-allow-origin:
      - '*'
      content-length:
      - '13904'
      content-type:
      - application/atom+xml; charset=UTF-8
      server:
      - Apache
      via:
      - 1.1 varnish, 1.1 varnish, 1.1 varnish
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: http://export.arxiv.org/api/query?max_results=5&search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+AND+all%3Amachine+learning&sortBy=submittedDate&sortOrder=descending&start=0
  response:
    body:
      string: ''
    headers:
      Accept-Ranges:
      - bytes
      Connection:
      - close
      Content-Length:
      - '0'
      Date:
      - Thu, 30 Oct 2025 11:09:57 GMT
      Location:
      - https://export.arxiv.org/api/query?search_query=cat:cs.AI%20OR%20cat:cs.LG%20OR%20cat:cs.CL%20AND%20all:machine%20learning&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending
      Retry-After:
      - '0'
      Server:
      - Varnish
      Strict-Transport-Security:
      - max-age=300
      Via:
      - 1.1 varnish
      X-Cache:
      - HIT
      X-Cache-Hits:
      - '0'
      X-Served-By:
      - cache-itm1220064-ITM
      X-Timer:
      - S1761822598.709184,VS0,VE0
    status:
      code: 301
      message: Moved Permanently
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: https://localhost:33210/api/query?max_results=5&search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+AND+all%3Amachine+learning&sortBy=submittedDate&sortOrder=descending&start=0
  response:
    body:
      string: "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n
        \ <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%20OR%20cat%3Acs.LG%20OR%20cat%3Acs.CL%20AND%20all%3Amachine%20learning%26id_list%3D%26start%3D0%26max_results%3D5\"
        rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv
        Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL AND all:machine learning&amp;id_list=&amp;start=0&amp;max_results=5</title>\n
        \ <id>http://arxiv.org/api/D9jbOwMlxm13ZRJT6OVevFlaMRw</id>\n  <updated>2025-10-30T00:00:00-04:00</updated>\n
        \ <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">33936</opensearch:totalResults>\n
        \ <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n
        \ <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">5</opensearch:itemsPerPage>\n
        \ <entry>\n    <id>http://arxiv.org/abs/2510.25701v1</id>\n    <updated>2025-10-29T17:05:00Z</updated>\n
        \   <published>2025-10-29T17:05:00Z</published>\n    <title>Interpreting LLMs
        as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical
        ML?</title>\n    <summary>  Large Language Models (LLMs) are increasingly
        explored as flexible\nalternatives to classical machine learning models for
        classification tasks\nthrough zero-shot prompting. However, their suitability
        for structured tabular\ndata remains underexplored, especially in high-stakes
        financial applications\nsuch as financial risk assessment. This study conducts
        a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM,
        a state-of-the-art\ngradient-boosting model, on a real-world loan default
        prediction task. We\nevaluate their predictive performance, analyze feature
        attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations.
        While LLMs are\nable to identify key financial risk indicators, their feature
        importance\nrankings diverge notably from LightGBM, and their self-explanations
        often fail\nto align with empirical SHAP attributions. These findings highlight
        the\nlimitations of LLMs as standalone models for structured financial risk\nprediction
        and raise concerns about the trustworthiness of their self-generated\nexplanations.
        Our results underscore the need for explainability audits,\nbaseline comparisons
        with interpretable models, and human-in-the-loop oversight\nwhen deploying
        LLMs in risk-sensitive financial environments.\n</summary>\n    <author>\n
        \     <name>Saeed AlMarri</name>\n    </author>\n    <author>\n      <name>Kristof
        Juhasz</name>\n    </author>\n    <author>\n      <name>Mathieu Ravaut</name>\n
        \   </author>\n    <author>\n      <name>Gautier Marti</name>\n    </author>\n
        \   <author>\n      <name>Hamdan Al Ahbabi</name>\n    </author>\n    <author>\n
        \     <name>Ibrahim Elfadel</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8
        pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop</arxiv:comment>\n    <link
        href=\"http://arxiv.org/abs/2510.25701v1\" rel=\"alternate\" type=\"text/html\"/>\n
        \   <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25701v1\" rel=\"related\"
        type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25626v1</id>\n
        \   <updated>2025-10-29T15:30:31Z</updated>\n    <published>2025-10-29T15:30:31Z</published>\n
        \   <title>Are Language Models Efficient Reasoners? A Perspective from Logic\n
        \ Programming</title>\n    <summary>  Modern language models (LMs) exhibit
        strong deductive reasoning capabilities,\nyet standard evaluations emphasize
        correctness while overlooking a key aspect\nof human-like reasoning: efficiency.
        In real-world reasoning scenarios, much of\nthe available information is irrelevant,
        and effective deductive inference\nrequires identifying and ignoring such
        distractions. We propose a framework for\nassessing LM reasoning efficiency
        through the lens of logic programming,\nintroducing a simple method to align
        proofs written in natural language -- as\ngenerated by an LM -- with shortest
        proofs found by executing the logic\nprogram. Efficiency is quantified by
        measuring how well a model avoids\nunnecessary inference. Empirically, we
        construct a dataset of math word\nproblems injected with various number of
        irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We
        find that current LMs show marked\naccuracy declines under such conditions
        -- even with minimal, domain-consistent\ndistractions -- and the proofs they
        generate frequently exhibit detours through\nirrelevant inferences.\n</summary>\n
        \   <author>\n      <name>Andreas Opedal</name>\n    </author>\n    <author>\n
        \     <name>Yanick Zengaffinen</name>\n    </author>\n    <author>\n      <name>Haruki
        Shirakami</name>\n    </author>\n    <author>\n      <name>Clemente Pasti</name>\n
        \   </author>\n    <author>\n      <name>Mrinmaya Sachan</name>\n    </author>\n
        \   <author>\n      <name>Abulhair Saparov</name>\n    </author>\n    <author>\n
        \     <name>Ryan Cotterell</name>\n    </author>\n    <author>\n      <name>Bernhard
        Sch\xF6lkopf</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted
        to NeurIPS 2025</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2510.25626v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25626v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25557v1</id>\n    <updated>2025-10-29T14:21:49Z</updated>\n
        \   <published>2025-10-29T14:21:49Z</published>\n    <title>Hybrid Quantum-Classical
        Recurrent Neural Networks</title>\n    <summary>  We present a hybrid quantum-classical
        recurrent neural network (QRNN)\narchitecture in which the entire recurrent
        core is realized as a parametrized\nquantum circuit (PQC) controlled by a
        classical feedforward network. The hidden\nstate is the quantum state of an
        $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$.
        The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving
        without external constraints.\nAt each timestep, mid-circuit readouts are
        combined with the input embedding\nand processed by the feedforward network,
        which provides explicit classical\nnonlinearity. The outputs parametrize the
        PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact
        and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity
        memory, (ii) partial\nobservation via mid-circuit measurements, and (iii)
        nonlinear classical control\nfor input-conditioned parametrization. We evaluate
        the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST,
        permuted MNIST, copying memory,\nand language modeling, adopting projective
        measurements as a limiting case to\nobtain mid-circuit readouts while maintaining
        a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism
        over the mid-circuit\nreadouts in a sequence-to-sequence model and show its
        effectiveness for machine\ntranslation. To our knowledge, this is the first
        model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive
        performance against\nstrong classical baselines across a broad class of sequence-learning
        tasks.\n</summary>\n    <author>\n      <name>Wenduan Xu</name>\n    </author>\n
        \   <link href=\"http://arxiv.org/abs/2510.25557v1\" rel=\"alternate\" type=\"text/html\"/>\n
        \   <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25557v1\" rel=\"related\"
        type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25232v1</id>\n    <updated>2025-10-29T07:18:43Z</updated>\n
        \   <published>2025-10-29T07:18:43Z</published>\n    <title>From Medical Records
        to Diagnostic Dialogues: A Clinical-Grounded\n  Approach and Dataset for Psychiatric
        Comorbidity</title>\n    <summary>  Psychiatric comorbidity is clinically
        significant yet challenging due to the\ncomplexity of multiple co-occurring
        disorders. To address this, we develop a\nnovel approach integrating synthetic
        patient electronic medical record (EMR)\nconstruction and multi-agent diagnostic
        dialogue generation. We create 502\nsynthetic EMRs for common comorbid conditions
        using a pipeline that ensures\nclinical relevance and diversity. Our multi-agent
        framework transfers the\nclinical interview protocol into a hierarchical state
        machine and context tree,\nsupporting over 130 diagnostic states while maintaining
        clinical standards.\nThrough this rigorous process, we construct PsyCoTalk,
        the first large-scale\ndialogue dataset supporting comorbidity, containing
        3,000 multi-turn diagnostic\ndialogues validated by psychiatrists. This dataset
        enhances diagnostic accuracy\nand treatment planning, offering a valuable
        resource for psychiatric\ncomorbidity research. Compared to real-world clinical
        transcripts, PsyCoTalk\nexhibits high structural and linguistic fidelity in
        terms of dialogue length,\ntoken distribution, and diagnostic reasoning strategies.
        Licensed psychiatrists\nconfirm the realism and diagnostic validity of the
        dialogues. This dataset\nenables the development and evaluation of models
        capable of multi-disorder\npsychiatric screening in a single conversational
        pass.\n</summary>\n    <author>\n      <name>Tianxi Wan</name>\n    </author>\n
        \   <author>\n      <name>Jiaming Luo</name>\n    </author>\n    <author>\n
        \     <name>Siyuan Chen</name>\n    </author>\n    <author>\n      <name>Kunyao
        Lan</name>\n    </author>\n    <author>\n      <name>Jianhua Chen</name>\n
        \   </author>\n    <author>\n      <name>Haiyang Geng</name>\n    </author>\n
        \   <author>\n      <name>Mengyue Wu</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2510.25232v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25232v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.25206v1</id>\n    <updated>2025-10-29T06:18:37Z</updated>\n
        \   <published>2025-10-29T06:18:37Z</published>\n    <title>RAVR: Reference-Answer-guided
        Variational Reasoning for Large Language\n  Models</title>\n    <summary>
        \ Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage
        models (LLMs), but critically depends on a key prerequisite: the LLM\ncan
        already generate high-utility reasoning paths with non-negligible\nprobability.
        For tasks beyond the LLM's current competence, such reasoning path\ncan be
        hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning.
        We are motivated by the insight from cognitive science that Why is\nthis the
        answer is often an easier question than What is the answer, as it\navoids
        the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory
        reconstruction-systematically retracing the reasoning that links a\nquestion
        to its answer. We show that LLMs can similarly leverage answers to\nderive
        high-quality reasoning paths. We formalize this phenomenon and prove\nthat
        conditioning on answer provably increases the expected utility of sampled\nreasoning
        paths, thereby transforming intractable problems into learnable ones.\nBuilding
        on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning),
        an end-to-end framework that uses answer-conditioned\nreasoning as a variational
        surrogate for question-only reasoning. Experiments\nin both general and math
        domains demonstrate consistent improvements over\nstrong baselines. We further
        analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens
        conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.\n</summary>\n
        \   <author>\n      <name>Tianqianjin Lin</name>\n    </author>\n    <author>\n
        \     <name>Xi Zhao</name>\n    </author>\n    <author>\n      <name>Xingyao
        Zhang</name>\n    </author>\n    <author>\n      <name>Rujiao Long</name>\n
        \   </author>\n    <author>\n      <name>Yi Xu</name>\n    </author>\n    <author>\n
        \     <name>Zhuoren Jiang</name>\n    </author>\n    <author>\n      <name>Wenbo
        Su</name>\n    </author>\n    <author>\n      <name>Bo Zheng</name>\n    </author>\n
        \   <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages,
        11 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2510.25206v1\"
        rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2510.25206v1\"
        rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\"
        term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\"
        scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \   <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\n
        \ </entry>\n</feed>\n"
    headers:
      Accept-Ranges:
      - bytes
      Age:
      - '1044'
      Connection:
      - keep-alive
      Date:
      - Thu, 30 Oct 2025 11:10:00 GMT
      Strict-Transport-Security:
      - max-age=300
      Vary:
      - Accept-Encoding
      X-Cache:
      - MISS, MISS, HIT
      X-Cache-Hits:
      - 0, 0, 0
      X-Served-By:
      - cache-lga21937-LGA, cache-lga21922-LGA, cache-bur-kbur8200068-BUR
      X-Timer:
      - S1761822601.741323,VS0,VE1
      access-control-allow-origin:
      - '*'
      content-length:
      - '13904'
      content-type:
      - application/atom+xml; charset=UTF-8
      server:
      - Apache
      via:
      - 1.1 varnish, 1.1 varnish, 1.1 varnish
    status:
      code: 200
      message: OK
version: 1
